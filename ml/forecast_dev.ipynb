{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Forecasting Model\n",
    "\n",
    "O'Reilly Machine Learning and Security\n",
    "* https://github.com/oreilly-mlsec/mlsec.net\n",
    "\n",
    "Chapter 3 Resources\n",
    "* https://github.com/oreilly-mlsec/book-resources/blob/master/chapter3/lstm-anomaly-detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not load module netifaces: No module named 'netifaces'\n",
      "WARNING: No route found for IPv6 destination :: (no default route?). This affects only IPv6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "standardizing data... (impossible given protocol is invariable)\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/praty/miniconda3/envs/cs210/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data...\")\n",
    "reader = read_tcpdump_file('data/week1_friday.tcpdump')\n",
    "packets = np.array([f for f in featurize_packets(reader)])\n",
    "\n",
    "print(\"standardizing data... (impossible given protocol is invariable)\")\n",
    "means = np.apply_along_axis(np.mean, 0, packets)\n",
    "stds = np.apply_along_axis(np.std, 0, packets)\n",
    "packets -= means\n",
    "packets /= stds\n",
    "\n",
    "print(\"done.\")\n",
    "\n",
    "# TODO: cull non-continuous variables OR manually define loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 50\n",
    "sequence_length = 4\n",
    "features = 14\n",
    "mean_window = 40\n",
    "loss_tolerance = 2\n",
    "train_test_split = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = self.generate_model()\n",
    "    \n",
    "    def generate_model(self):\n",
    "        \n",
    "        layers = [\n",
    "            tf.keras.layers.LSTM(input_shape=(sequence_length - 1, features), \n",
    "                                 units=32, \n",
    "                                 return_sequences=True),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.LSTM(units=128,\n",
    "                                 return_sequences=True),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.LSTM(units=100,\n",
    "                                 return_sequences=False),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(units=features),\n",
    "            tf.keras.layers.Activation('linear')\n",
    "        ]\n",
    "        model = tf.keras.Sequential(layers)\n",
    "        model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_data(self, data, training=True):\n",
    "        \n",
    "        if training:\n",
    "\n",
    "            print('creating train n-grams...')\n",
    "\n",
    "            train_grams = []\n",
    "            for i in range(0, len(data) - sequence_length):\n",
    "                train_grams.append(data[i: i + sequence_length])\n",
    "            train_grams = np.array(train_grams)\n",
    "\n",
    "            print('train data shape : ', train_grams.shape)\n",
    "\n",
    "            self.x_train = train_grams[:, :-1]\n",
    "            self.y_train = train_grams[:, -1]\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('creating test n-grams...')\n",
    "\n",
    "            test_grams = []\n",
    "            for i in range(0, len(data) - sequence_length):\n",
    "                test_grams.append(data[i: i + sequence_length])\n",
    "            test_grams = np.array(test_grams)\n",
    "\n",
    "            print('test data shape : ', test_grams.shape)  \n",
    "\n",
    "            self.x_test = test_grams[:, :-1]\n",
    "            self.y_test = test_grams[:, -1]        \n",
    "    \n",
    "    def train(self, data):\n",
    "        \n",
    "        self.prepare_data(data)\n",
    "        self.train_history = self.model.fit(self.x_train, self.y_train,\n",
    "                                            batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "    def test(self, data):\n",
    "        \n",
    "        assert self.train_history is not None \n",
    "        \n",
    "        self.prepare_data(data, training=False)\n",
    "        \n",
    "        losses_in_window = deque()\n",
    "        moving_mean = 0\n",
    "        rolling_std = 0\n",
    "        \n",
    "        for i in range(self.x_test.shape[0]):\n",
    "            \n",
    "            test_loss = self.model.evaluate(x=np.expand_dims(self.x_test[i], 0), \n",
    "                                            y=np.expand_dims(self.y_test[i], 0), batch_size=1)\n",
    "            \n",
    "            if i < mean_window:\n",
    "                moving_mean += test_loss / mean_window\n",
    "                losses_in_window.append(test_loss)\n",
    "                print(\"build mean\")\n",
    "            else:\n",
    "                moving_mean += (test_loss - losses_in_window[0]) / mean_window\n",
    "                losses_in_window.popleft()\n",
    "                losses_in_window.append(test_loss)\n",
    "                \n",
    "                # not efficient\n",
    "                rolling_std = np.std(losses_in_window)\n",
    "                \n",
    "                if np.abs(test_loss - moving_mean) < rolling_std * loss_tolerance:\n",
    "                    print(\"all clear\")\n",
    "                else:\n",
    "                    print(\"\\nMALICIOUS\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train n-grams...\n",
      "train data shape :  (1010221, 4, 14)\n",
      "Epoch 1/10\n",
      "1010221/1010221 [==============================] - 256s 253us/step - loss: nan\n",
      "Epoch 2/10\n",
      "1010221/1010221 [==============================] - 285s 282us/step - loss: nan\n",
      "Epoch 3/10\n",
      " 742500/1010221 [=====================>........] - ETA: 1:05 - loss: nan"
     ]
    }
   ],
   "source": [
    "model = ForecastModel()\n",
    "\n",
    "split = (len(packets) * train_test_split) // 10\n",
    "\n",
    "model.train(packets[:split])\n",
    "model.test(packets[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
